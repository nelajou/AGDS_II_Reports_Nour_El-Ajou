---
title: "soil_tutorial"
author: "Nour El-Ajou"
date: "2023-11-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

# 1 Data download.


```{r}
library(tidyverse)

sessionInfo()

# Load soil data from sampling locations
df_obs <- readr::read_csv(
  here::here("data/berne_soil_sampling_locations.csv")
)

# Display data
head(df_obs) |> 
  knitr::kable()

```

# 5.1 Simple Random Forest model

Re-implement the digital soil mapping workflow, using Random Forest, as demonstrated in this tutorial, but for the binary categorical variable waterlog.100. Here are a few hints as a guide:

Make sure that the categorical target variable is encoded as a factor using the function factor().

Start with a model that includes all predictors, trained on the pre-defined training subset.

Evaluate the model on the testing subset of the data

Consider appropriate metrics as described in AGDS Book Chapter 8.3.

Is the data balanced in terms of observed TRUE and FALSE values?



What does this imply for the interpretation of the different metrics?

## Raster files
```{r}
# Get a list with the path to all raster files
list_raster <- list.files(
  here::here("data/covariates"),
  full.names = TRUE
  )
```


```{r}
# Display data (lapply to clean names)
lapply(
  list_raster, 
  function(x) sub(".*/(.*)", "\\1", x)
  ) |> 
  unlist() |> 
  head(5) |> 
  print()
```


```{r}
# Load a raster file as example: Picking the slope profile at 2 m resolution
raster_example <- terra::rast(
  here::here("data/covariates/Se_slope2m.tif")
  )
raster_example
```

```{r}
# Load all files as one batch
all_rasters <- terra::rast(list_raster)
all_rasters
```
## assemble full dataset
```{r}
# Extract coordinates from sampling locations
sampling_xy <- df_obs |> 
  dplyr::select(x, y)

# From all rasters, extract values for sampling coordinates
df_covars <- terra::extract(
  all_rasters,  # The raster we want to extract from
  sampling_xy,  # A matrix of x and y values to extract for
  ID = FALSE    # To not add a default ID column to the output
  )

df_full <- cbind(df_obs, df_covars)

head(df_full) |> 
  knitr::kable() 
```

```{r}
vars_categorical <- df_covars |> 
  
  # Get number of distinct values per variable
  dplyr::summarise(dplyr::across(dplyr::everything(), ~dplyr::n_distinct(.))) |> 
  
  # Turn df into long format for easy filtering
  tidyr::pivot_longer(
    dplyr::everything(), 
    names_to = "variable", 
    values_to = "n"
    ) |> 
  
  # Filter out variables with 10 or less distinct values
  dplyr::filter(n <= 10) |>
  
  # Extract the names of these variables
  dplyr::pull('variable')

cat("Variables with less than 10 distinct values:", 
    ifelse(length(vars_categorical) == 0, "none", vars_categorical))
```

```{r}
df_full <- df_full |> 
  dplyr::mutate(dplyr::across(all_of(vars_categorical), ~as.factor(.)))
```


## missing values

```{r}
# Get number of rows to calculate percentages
n_rows <- nrow(df_full)

# Get number of distinct values per variable
df_full |> 
  dplyr::summarise(dplyr::across(dplyr::everything(), 
                                 ~ length(.) - sum(is.na(.)))) |> 
  tidyr::pivot_longer(dplyr::everything(), 
                      names_to = "variable", 
                      values_to = "n") |>
  dplyr::mutate(perc_available = round(n / n_rows * 100)) |> 
  dplyr::arrange(perc_available) |> 
  head(10) |> 
  knitr::kable()
```

```{r}
df_full |> 
  dplyr::select(1:20) |>   # reduce data for readability of the plot
  visdat::vis_miss()
```


```{r}
# if (!dir.exists(here::here("data"))) system(paste0("mkdir ", here::here("data")))
# saveRDS(df_full, 
        # here::here("data/df_full.rds"))
```


 Train a random forest

## Definition of Targets and Predictors
```{r}
df_full <- readRDS(here::here("data/df_full.rds"))

head(df_full) |> 
  knitr::kable()
```

```{r}
# Specify target: The pH in the top 10cm
target <- "waterlog.100"
df_full <- subset(df_full, select = - c(vszone) )

# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]


cat("The target is:", target,
    "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "...")
```
## Training and Test Dataset

```{r}
df_full$waterlog.100 <- as.factor(df_full$waterlog.100)

# Split dataset into training and testing sets
df_train <- df_full |> dplyr::filter(dataset == "calibration")
df_test  <- df_full |> dplyr::filter(dataset == "validation")

# Filter out any NA to avoid error when running a Random Forest
df_train <- df_train |> tidyr::drop_na()
df_test <- df_test   |> tidyr::drop_na()

# A little bit of verbose output:
n_tot <- nrow(df_train) + nrow(df_test)

perc_cal <- (nrow(df_train) / n_tot) |> round(2) * 100
perc_val <- (nrow(df_test)  / n_tot) |> round(2) * 100

cat("For model training, we have a calibration / validation split of: ",
    perc_cal, "/", perc_val, "%")
```
## Train basic forest

```{r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_basic <- ranger::ranger( 
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  seed = 42,                    # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# Print a summary of fitted model
print(rf_basic)
```
```{r}
# Load area to be predicted
raster_mask <- terra::rast(here::here("data/area_to_be_mapped.tif"))


# Turn target raster into a dataframe, 1 px = 1 cell
df_mask <- as.data.frame(raster_mask, xy = TRUE)

# Filter only for area of interest
df_mask <- df_mask |> 
  dplyr::filter(area_to_be_mapped == 1)

# Display df
head(df_mask) |> 
  knitr::kable()
```


```{r}
files_covariates <- list.files(
  path = here::here("data/covariates/"), 
  pattern = ".tif$",
  recursive = TRUE, 
  full.names = TRUE
  )

```


```{r}
 random_files <- sample(files_covariates, 2)
terra::rast(random_files[1])
```
## Select Raster and covariates - basic Forest

```{r}
# Filter that list only for the variables used in the RF
preds_selected <- names(df_train[ ,predictors_all])
files_selected <- files_covariates[apply(sapply(X = preds_selected,
                                            FUN = grepl,
                                            files_covariates),
                                     MARGIN =  1,
                                     FUN = any)]

# Load all rasters as a stack
raster_covariates <- terra::rast(files_selected)

```


## Get coordinates for covariates - Basic Forest
```{r}
# Get coordinates for which we want data
df_locations <- df_mask |> 
  dplyr::select(x, y)

# Extract data from covariate raster stack for all gridcells in the raster
df_predict <- terra::extract(
  raster_covariates,   # The raster we want to extract from
  df_locations,        # A matrix of x and y values to extract for
  ID = FALSE           # To not add a default ID column to the output
  )

df_predict <- cbind(df_locations, df_predict) |> 
  tidyr::drop_na()  # Se_TWI2m has a small number of missing data
```


## Generate prediction and add it to test-set - Basic Forest
```{r}
# Need to load {ranger} because ranger-object is used in predict()
library(ranger) 

# Make predictions for validation sites
prediction <- predict(
  rf_basic,           # RF model
  data = df_test,   # Predictor data
  num.threads = parallel::detectCores() - 1
  )

# Save predictions to validation df
df_test$pred <- prediction$predictions

```
##  predictions with Prediction set - Basic Forest

```{r}


# Make predictions using the RF model
prediction <- predict(
  rf_basic,              # RF model
  data = df_predict,   
  num.threads = parallel::detectCores() - 1
  )


# Attach predictions to dataframe and round them
df_predict$prediction <- prediction$predictions
```


## Extract data frame and raster transformation - Basic Forest
```{r}
# Extract dataframe with coordinates and predictions
df_map <- df_predict |>
  dplyr::select(x, y, prediction)

# Turn dataframe into a raster
raster_pred <- terra::rast(
  df_map,                  # Table to be transformed
  crs = "+init=epsg:2056", # Swiss coordinate system
  extent = terra::ext(raster_covariates) # Prescribe same extent as predictor rasters
  )
```

## Plot Prediction map - Basic Forest

```{r}
# Let's have a look at our predictions!
# To have some more flexibility, we can plot this in the ggplot-style as such:
ggplot2::ggplot() +
  tidyterra::geom_spatraster(data = raster_pred) +
  ggplot2::scale_fill_viridis_c(
    na.value = NA,
    option = "viridis",
    name = "pH"
    ) +
  ggplot2::theme_classic() +
  ggplot2::scale_x_continuous(expand = c(0, 0)) +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(title = "Predicted waterlog at 100 cm depth, binary variable")
```


```{r}
# Save raster as .tif file
terra::writeRaster(
  raster_pred,
  "../data/ra_predicted_ph0-10.tif",
  datatype = "FLT4S",  # FLT4S for floats, INT1U for integers (smaller file)
  filetype = "GTiff",  # GeoTiff format
  overwrite = TRUE     # Overwrite existing file
)
```


Evaluate the model on the testing subset of the data. Consider appropriate metrics as described in AGDS Book Chapter 8.3. Is the data balanced in terms of observed TRUE and FALSE values? What does this imply for the interpretation of the different metrics?

#### To do: Check for balance in dataset

```{r}
```

```{r}
for(i in 1:length(predictors_all) ){
plot(df_full[,"waterlog.100"], df_full[,predictors_all[i]])
}
```




#confusion matrix - basic case 

```{r}
# Make classification predictions
y <- as.factor(df_test$waterlog.100)
x <- df_test$pred # Use 0.5 as threshold

# Change class names
levels(y) <- levels(x) <- c("no", "yes")

# plot confusion matrix
conf_matrix_basic <- caret::confusionMatrix(data = x, reference = y)
conf_matrix_basic
```

Consider appropriate metrics as described in AGDS Book Chapter 8.3. Is the data balanced in terms of observed TRUE and FALSE values? 



```{r}
tp_basic <- conf_matrix_basic$table[2,2]
fp_basic <- conf_matrix_basic$table[2,1]

tn_basic <- conf_matrix_basic$table[1,1]
fn_basic <- conf_matrix_basic$table[1,2]
```

Accuracy = (TP + TN )/ N
```{r}
acc_basic <- na.omit( (tp_basic + tn_basic )/ sum(conf_matrix_basic$table) )
cat("the accuracy for the basic model is ",acc_basic)
```


Precision = TP / (TP + FP)
```{r}
prec_basic <- na.omit(tp_basic / (tp_basic + fp_basic) )
cat("the precision for the basic model is ",prec_basic)
```

f1 <- 2TP / (2TP + FP + FN)
```{r}
f1_basic <-  na.omit(2*tp_basic / (2*tp_basic + fp_basic +fn_basic) )
cat("the f1 for the basic model is ",f1_basic)

```






# 5.2 Variable selection (Boruta)

## 5.2.1
Reduce the predictor set as demonstrated in this tutorial.

### Variable importance basic model
```{r}
# Let's run the basic model again but with recording the variable importance
rf_basic <- ranger::ranger( 
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all],   # Predictor variables
  importance   = "permutation", # Pick permutation to calculate variable importance
  seed = 42,                    # Specify seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# Extract the variable importance and create a long tibble
vi_rf_basic <- rf_basic$variable.importance |>
  dplyr::bind_rows() |> 
  tidyr::pivot_longer(cols = dplyr::everything(), names_to = "variable")

# Plot variable importance, ordered by decreasing value
gg <- vi_rf_basic |> 
  ggplot2::ggplot(ggplot2::aes(x = reorder(variable, value), y = value)) +
  ggplot2::geom_bar(stat = "identity", fill = "grey50", width = 0.75) + 
  ggplot2::labs(
    y = "Change in OOB MSE after permutation", 
    x = "",
    title = "Variable importance based on OOB") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip() + ggplot2::theme(axis.text.y = ggplot2::element_text(angle = 45, hjust = 1) )
        
vi_rf_basic
# Display plot
# gg
```

## 5.2.2 Run Boruta Algorithm

```{r}

set.seed(42)

# run the algorithm
bor <- Boruta::Boruta(
    y = df_train[, target],
    x = df_train[, predictors_all],
    maxRuns = 50, # Number of iterations. Set to 30 or lower if it takes too long
    num.threads = parallel::detectCores()-1)

# obtain results: a data frame with all variables, ordered by their importance
df_bor <- Boruta::attStats(bor) |>
  tibble::rownames_to_column() |>
  dplyr::arrange(dplyr::desc(meanImp))

# plot the importance result
ggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp),
                             y = meanImp,
                             fill = decision),
                data = df_bor) +
  ggplot2::geom_bar(stat = "identity", width = 0.75) +
  ggplot2::scale_fill_manual(values = c("grey30", "tomato", "grey70")) +
  ggplot2::labs(
    y = "Variable importance",
    x = "",
    title = "Variable importance based on Boruta") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()

```


## retrain RF with Boruta Selection
```{r}
# get retained important variables
predictors_selected <- df_bor |>
  dplyr::filter(decision == "Confirmed") |>
  dplyr::pull(rowname)

length(predictors_selected)
```

```{r}
# re-train Random Forest model
rf_bor <- ranger::ranger(
  y = df_train[, target],              # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42,                           # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_bor
```

# Save forests and Datasets

```{r}
# Save relevant data for model testing in the next chapter.

saveRDS(rf_bor,                   
      here::here("data/rf_bor_wat.10.rds"))

saveRDS(df_train[, c(target, predictors_selected)],
  here::here("data/cal_bor_wat.10.rds"))
 
saveRDS(df_test[, c(target, predictors_selected)],
   here::here("data/val_bor_wat.10.rds"))



saveRDS(rf_basic,                   
   here::here("data/rf_wat.10.rds"))

saveRDS(df_train[, c(target, predictors_all)],
  here::here("data/cal_wat.10.rds"))
 
saveRDS(df_test[, c(target, predictors_all)],
   here::here("data/val_wat.10.rds"))

```


 Model Analysis


## Load Data, Forests and Prediction Area
```{r}
# Load random forest model
rf_bor   <- readRDS(here::here("data/rf_bor_wat.10.rds"))
df_train_bor <- readRDS(here::here("data/cal_bor_wat.10.rds"))
df_test_bor  <- readRDS(here::here("data/val_bor_wat.10.rds"))


rf_basic <- readRDS(here::here("data/rf_wat.10.rds"))
df_train <- readRDS(here::here("data/cal_wat.10.rds"))
df_test  <- readRDS(here::here("data/val_wat.10.rds"))

```


## Select Raster and covariates - Boruta Forest
```{r}
# Filter that list only for the variables used in the RF
preds_selected_bor <- names(df_train[ ,predictors_selected])
files_selected_bor <- files_covariates[apply(sapply(X = preds_selected_bor,
                                            FUN = grepl,
                                            files_covariates),
                                     MARGIN =  1,
                                     FUN = any)]

# Load all rasters as a stack
raster_covariates_bor <- terra::rast(files_selected_bor)
```


## Get coordinates for covariates - Boruta Forest
```{r}
# Get coordinates for which we want data
df_locations <- df_mask |> 
  dplyr::select(x, y)

# Extract data from covariate raster stack for all gridcells in the raster
df_predict_bor <- terra::extract(
  raster_covariates_bor,   # The raster we want to extract from
  df_locations,        # A matrix of x and y values to extract for
  ID = FALSE           # To not add a default ID column to the output
  )

df_predict_bor <- cbind(df_locations, df_predict_bor) |> 
  tidyr::drop_na()  # Se_TWI2m has a small number of missing data
```



## Generate prediction and add it to test-set - Boruta Forest
```{r}
 # Need to load {ranger} because ranger-object is used in predict()
library(ranger) 
 
 # Make predictions for validation sites
 prediction_bor <- predict(
   rf_bor,           # RF model
   data = df_test_bor,   # Predictor data
   num.threads = parallel::detectCores() - 1
   )

# Save predictions to validation df
 df_test_bor$pred <- prediction_bor$predictions
```


##  predictions with Prediction set - Boruta Forest
```{r}
# Make predictions using the RF model
prediction_bor <- predict(
  rf_bor,              # RF model
  data = df_predict_bor,   
  num.threads = parallel::detectCores() - 1
  )


# Attach predictions to dataframe and round them
df_predict_bor$prediction <- prediction_bor$predictions
```



## Extract data frame and raster transformation - Boruta Forest

```{r}
# Extract dataframe with coordinates and predictions
df_map_bor <- df_predict_bor|>
  dplyr::select(x, y, prediction)

# Turn dataframe into a raster
raster_pred_bor <- terra::rast(
  df_map_bor,                  # Table to be transformed
  crs = "+init=epsg:2056", # Swiss coordinate system
  extent = terra::ext(raster_covariates_bor) # Prescribe same extent as predictor ras
  )

```

## Plot Prediction map - Boruta Forest

```{r}
# Let's have a look at our predictions!
# To have some more flexibility, we can plot this in the ggplot-style as such:
ggplot2::ggplot() +
  tidyterra::geom_spatraster(data = raster_pred_bor) +
  ggplot2::scale_fill_viridis_c(
    na.value = NA,
    option = "viridis",
    name = "pH"
    ) +
  ggplot2::theme_classic() +
  ggplot2::scale_x_continuous(expand = c(0, 0)) +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(title = "Predicted waterlog at 100 cm depth, binary variable")
```


```{r}
terra::writeRaster(
  raster_pred_bor,
  "../data/ra_predicted_bor_ph0-10.tif",
  datatype = "FLT4S",  # FLT4S for floats, INT1U for integers (smaller file)
  filetype = "GTiff",  # GeoTiff format
  overwrite = TRUE     # Overwrite existing file
)

```



## Confusion matrix - boruta Forest

```{r}
# Make classification predictions
y_bor <- as.factor(df_test_bor$waterlog.100)
x_bor <- df_test_bor$pred # Use 0.5 as threshold

# Change class names
levels(y_bor) <- levels(x_bor) <- c("no", "yes")

# plot confusion matrix
conf_matrix_bor <- caret::confusionMatrix(data = x_bor, reference = y_bor)
conf_matrix_bor$table
```
```{r}
tp_bor <- conf_matrix_bor$table[2,2]
fp_bor <- conf_matrix_bor$table[2,1]

tn_bor <- conf_matrix_bor$table[1,1]
fn_bor <- conf_matrix_bor$table[1,2]
```


```{r}
acc_bor <- na.omit( (tp_bor + tn_bor )/ sum(conf_matrix_bor$table) )
cat("the accuracy for the boruta model is ",acc_bor)
```


Precision = TP / (TP + FP)
```{r}
prec_bor <- na.omit(tp_bor / (tp_bor + fp_bor) )
prec_bor
```

f1 <- 2TP / (2TP + FP + FN)
```{r}
f1_bor <-  na.omit(2*tp_bor / (2*tp_bor + fp_bor +fn_bor) )
f1_bor
```


```{r}
for(i in 1:length(predictors_selected)) {
plot(df_full[,"waterlog.100"], df_full[,predictors_selected[i]])
}
```
##### To do: Check for balance in dataset


# 5.2.3 OBB prediction and model comparison


Repeat the model evaluation and compare the model performance on the test set with what was obtained with the model using all available covariates. Which model generalises better to unseen data?

Would the same model choice be made if we considered the OOB prediction error reported as part of the trained model object?


```{r}
# OOB prediction error of the final model
obb_basic <- sqrt(rf_basic$prediction.error)
obb_bor <- sqrt(rf_bor$prediction.error)
```


```{r}
cat("Looking at the prediction error, the Boruta Model would be preferred over the
basic case since its Prediction error with", sqrt(rf_bor$prediction.error), "smaller than the one of the basic model"
) 

```


## Comparison table for both models

```{r}
diag_table <- rbind(diag_basic <- c(round(obb_basic,3), round(acc_basic,3),                                              round(prec_basic,3),round(f1_basic,3) ),
                      diag_bor <-  c(round(obb_bor,3), round(acc_bor,3),                                              round(prec_bor,3),round(f1_bor,3) )
)
diag_table <- as.data.frame(diag_table, row.names = c("Basic RF", "Boruta RF") )
colnames(diag_table) <- c("OBB", "Accuracy", "Precision", "F1")

 better_model <- as.vector(
                 c( rownames(diag_table)[which.min(diag_table$OBB)],
                    rownames(diag_table ) [which.max(diag_table$Accuracy)],
                    rownames(diag_table) [which.max(diag_table$Precision)],
                    rownames(diag_table  )[which.max(diag_table$F1) ]  )  )
 

 diag_table <- rbind(diag_table, better_model)
 
rownames(diag_table) <- c("Basic RF", "Boruta RF", "Better Model")
diag_table


```
In any diagnosis estimate, the boruta Forest performs better over the Basic one, 
consequently we pfrefer this model over the other. 

### Check of the 2 predictions really are different

```{r}
summary(df_predict$prediction == df_predict_bor$prediction)
``` 

 #5.3 Model optimization

In AGDS Book Chapter 11, you learned how to optimize hyperparameters using cross-validation.


## 5.3.1 Hyperparameter tuning 

Using the training data subset, implement a 5-fold cross-validation to optimise the hyperparameters mtry and min.node.size of the same Random Forest model as implemented above. You may use the {caret} library as demonstrated in AGDS Book.

```{r}
# The same model formulation is in the previous chapter
pred_plus <- paste(predictors_selected,collapse  = " + ")
pp <- recipes::recipe(as.formula(paste0("waterlog.100 ~", pred_plus)), 
                      data = df_train_bor) 
```


```{r}
# Load required libraries
library(caret)
library(ranger)

# Set seed for reproducibility
set.seed(199)

# Define the hyperparameter grid
mtry_values <- seq(20,38,2)
min.node.size_values <- seq(10,30,2)
# Create a data frame to store results
results <- data.frame()

# Perform hyperparameter tuning with 5-fold cross-validation

  # Train the model using caret::train for classification
  mod <- caret::train(
    x = df_train_bor[, predictors_selected],
    y = df_train_bor[, target],
    method = "ranger",
    metric = "Accuracy",
    trControl = trainControl(
      method = "cv",
      number = 5,
      savePredictions = "final"
    ),
    tuneGrid = expand.grid(
      .mtry = mtry_values,
      .min.node.size = min.node.size_values,
      .splitrule = "gini"  # or "extratrees" or "entropy", depending on your preference
    ),
    replace = FALSE,
    sample.fraction = 0.5,
    num.trees = 100,
    seed = 1982
  )
```

## compare hyperparameters

```{r}
opt_row <- mod$results[which.max(mod$results$Accuracy),]
  opt_mtry          <- opt_row$mtry
  opt_min_node_size <- opt_row$min.node.size
 
  opt_row
  opt_mtry
  opt_min_node_size
```


```{r}
hyper.param.table <- 
  as.data.frame(
  rbind(
    c(opt_mtry,opt_min_node_size),
    c(rf_bor$mtry, rf_bor$min.node.size)
  )
)
rownames(hyper.param.table) <- c("tuned boruta RF" ,"boruta RF")
colnames(hyper.param.table) <- c("mtry", "minimum node size")
hyper.param.table
```
Comment: The optimal values vary quite strong when changing the random seed, so this should be alright. 

## 5.3.2 Evaluation of tuned vs. untuned model.

Evaluate the optimized model on the test set using the same metrics as considered above. Does the model generalise better to unseen data than the initial model (which used default hyperparameters, see ?ranger::ranger).


## Train new ranger model with hyperparameters set to the optimal ones. 

```{r}
# re-train Random Forest model
rf_bor_tuned <- ranger::ranger(
  y = df_train[, target],              # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42,                           # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1
  , mtry = opt_mtry, min.node.size = opt_min_node_size) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_bor_tuned

```
## Generate prediction and add it to test-set - Boruta tuned Forest

```{r}

 # Need to load {ranger} because ranger-object is used in predict()
library(ranger) 
 
 # Make predictions for validation sites
 prediction_bor_tuned <- predict(
   rf_bor_tuned,           # RF model
   data = df_test_bor,   # Predictor data
   num.threads = parallel::detectCores() - 1
   )

# Save predictions to validation df
 df_test_bor$pred_tuned <- prediction_bor_tuned$predictions
```




## Confusion matrix - boruta tuned Forest

```{r}
# Make classification predictions
y_bor <- as.factor(df_test_bor$waterlog.100)
x_bor <- df_test_bor$pred_tuned # Use 0.5 as threshold

# Change class names
levels(y_bor) <- levels(x_bor) <- c("no", "yes")

# plot confusion matrix
conf_matrix_bor_tuned <- caret::confusionMatrix(data = x_bor, reference = y_bor)
conf_matrix_bor_tuned$table

tp_bor_tuned <- conf_matrix_bor_tuned$table[2,2]
fp_bor_tuned <- conf_matrix_bor_tuned$table[2,1]

tn_bor_tuned <- conf_matrix_bor_tuned$table[1,1]
fn_bor_tuned <- conf_matrix_bor_tuned$table[1,2]
```


```{r}
# Accuracy = (TP + TN )/ N

acc_bor_tuned <- na.omit( (tp_bor_tuned + tn_bor_tuned )/ sum(conf_matrix_bor_tuned$table) )
cat("the accuracy for the tuned Boruta model is ",acc_bor_tuned)


# Precision = TP / (TP + FP)

prec_bor_tuned <- na.omit(tp_bor_tuned / (tp_bor_tuned + fp_bor_tuned) )
prec_bor_tuned

# f1 <- 2TP / (2TP + FP + FN)

f1_bor_tuned <-  na.omit(2*tp_bor_tuned / (2*tp_bor_tuned + fp_bor_tuned +fn_bor_tuned) )
f1_bor_tuned

# OOB prediction error of the final model
obb_bor_tuned <- sqrt(rf_bor_tuned$prediction.error)
```


## Comparison table for both models

```{r}
diag_table_3 <- rbind( c(round(obb_basic,3), round(acc_basic,3),                                              round(prec_basic,3),round(f1_basic,3) ),
   diag_bor <-  c(round(obb_bor,3), round(acc_bor,3),  
                  round(prec_bor,3), round(f1_bor,3) ),
       diag_bor_tuned <- c(round(obb_bor_tuned,3),  round(acc_bor_tuned,3),                                      round(prec_bor_tuned,3), round(f1_bor_tuned,3) )
)
diag_table_3 <- as.data.frame(diag_table_3)
rownames(diag_table_3) <- c("Basic RF", "Boruta RF","Boruta tuned")

colnames(diag_table_3) <- c("OBB", "Accuracy", "Precision", "F1")

 best_model <- as.vector(
                 c( rownames(diag_table_3)[which.min(diag_table_3$OBB)],
                    rownames(diag_table_3 ) [which.max(diag_table_3$Accuracy)],
                    rownames(diag_table_3) [which.max(diag_table_3$Precision)],
                    rownames(diag_table_3  )[which.max(diag_table_3$F1) ]  ) 
                 )
 

 diag_table_3 <- rbind(diag_table_3, best_model)
 
rownames(diag_table_3) <- c("Basic RF", "Boruta RF","Boruta tuned", "Best Model")
 

diag_table_3


```
Considering only the prediction error, the tuned Boruta model performs worse than the
untuned boruta model. Considering all other indicators, the tuned model performs better. 

##### To do: Precision AUC and F1 missing
##### To do: Check for balance in dataset
##### To do: Boruto_for diagnostics are == 0, maybe because of balance.


# 5.2.3 OBB prediction and model comparison


Repeat the model evaluation and compare the model performance on the test set with what was obtained with the model using all available covariates. Which model generalises better to unseen data?

Would the same model choice be made if we considered the OOB prediction error reported as part of the trained model object?


```{r}
# OOB prediction error of the final model
obb_basic <- sqrt(rf_basic$prediction.error)
obb_bor <- sqrt(rf_bor$prediction.error)
```


```{r}
cat("Looking at the prediction error, the Boruta Model would be preferred over the
basic case since its Prediction error with", sqrt(rf_bor$prediction.error), "smaller than the one of the basic model"
) 

```


```{r}
summary(df_predict$prediction == df_predict_bor$prediction)
``` 



# 5.4 Probabilistic predictions

## 5.4.1
Using the optimised (or if you didn’t manage - the initial default) hyperparameters, train the Random Forest model, setting ranger::ranger(..., probability = TRUE).
### RF_bor_prob

```{r}
# re-train Random Forest model
rf_bor_prob <- ranger::ranger(
  y = df_train[, target],              # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42,                           # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1,
  probability = TRUE,
  min.node.size = opt_min_node_size,
  mtry = opt_mtry) # Use all but one CPU core for quick model training


```


This yields not a model predicting a binary class, but a probability of the target to be TRUE. This lets the user chose where to put the threshold for translating a probability to a binary class. E.g., if the predicted probability is > 0.5
, then consider this as a prediction of TRUE. 



## Generate prediction and add it to test-set - Boruta Forest
```{r}
# Need to load {ranger} because ranger-object is used in predict()
library(ranger) 
 
 # Make predictions for validation sites
 prediction_bor_prob <- predict(
   rf_bor_prob,           # RF model
   data = df_test_bor,   # Predictor data
   num.threads = parallel::detectCores() - 1
   )

# Save predictions to validation df
 df_test_bor_prob <- df_test_bor
 df_test_bor_prob$pred <- prediction_bor_prob$predictions
```

## Prepare vectors for AOC Curve

```{r}
df_treshhold <- as.data.frame(cbind( 
  "waterlog.100" = as.numeric( as.character( df_test_bor_prob$waterlog.100 ) ), "P(waterlog = 1)" =   df_test_bor_prob$pred[,2]) )

 treshholds  <-  (seq(from = 1, to = 0, -0.001) )
 
 
true_pos <- rep(NA,length(treshholds))
false_pos <- rep(NA,length(treshholds))
all_pos <- rep(70,length(treshholds))
all_neg <- rep(130, length(treshholds))
```


```{r}
for(i in 1:length(treshholds) ){

df_treshhold$try <- df_treshhold$`P(waterlog = 1)`

df_treshhold$try[df_treshhold$try >= treshholds[i]] <- 1
df_treshhold$try[df_treshhold$try < treshholds[i]] <- 0


# Make classification predictions
y_bor <- as.factor(df_treshhold$waterlog.100)
x_bor <- as.factor(df_treshhold$try) # Use 0.5 as threshold

# Change class names
levels(y_bor) <- levels(x_bor) <- c("no", "yes")

# plot confusion matrix
conf_matrix_bor <- caret::confusionMatrix(data = x_bor, reference = y_bor)
conf_matrix_bor$table

# true positive rate
true_pos[i] <- conf_matrix_bor$table[2,2]
# False positive rate
false_pos[i] <- conf_matrix_bor$table[2,1]
}
```

ROC Plot
```{r}
true_pos_rate <- true_pos / all_pos
false_pos_rate <- false_pos / all_neg
plot(false_pos_rate, true_pos_rate, col = "black", type = "l", main = "ROC Curve", xlab = "False positive Rate", ylab = "True positive Rate", xlim = c(0,1), ylim = c(0,1) )
abline( v = 0.17, col = "red")
abline( v = 0.6, col = "red")
```

```{r}
# true_pos_rate
# false_pos_rate
```


### Establish the Reicever-operating-characteristic curve, as described in AGDS Book Chapter 8.3.



## 5.4.2

Consider you inform an infrastructure construction project where waterlogged soils severely jeopardize the stability of the building. Then, consider you inform a project where waterlogged soils are unwanted, but not critical. In both cases, your prediction map of a binary classification is used as a basis and the binary classification is derived from the probabilistic prediction.


### How would you chose the threshold in each case?

- analyse the weird graph


### Would you chose the same threshold in both cases? If not, explain why. 


I would not chose the same treshhold in both cases because in the first example, false negatives are to be avoided as much as possible. This implies that there will be more buildings that we declare to be "unsave" even though they would be save (more false positives)

In the second example on the other hand, they are not that critical and we can try to strike a balance in minimizing both, false positivs and false negatives. 


### Can you think of an analogy of a similarly-natured problem in another realm?

An analogy would be the testing of new drugs:

While testing a new drug for a serious disease, e.g. cancer, we want to minimize
Type I Errors, while Type II Errors are not very crucial. I.e. we only want to measure the effect of the drug given it exists (minimize false positives) and we accept,rejecting the use of the drug in some cases, event though it could help (we have more false negatives in return)

The other case would be the assesment of side effects of a new drug.

In that case we want to minimize false negatives, meaning we don't want any cases were we don't claim there are serious side-effect, even though they exist. At the same time we take into account that their will be more false positives, meaning cases were we predict side effects to occur, even though that won't be the case. 


# To dos:
-look at balance of dataset with dominic
-specifically check balance between training and testset
- go through entire code, clean it
- go through all exersices and look that you got everything. 
- compare clearly in a chunk, which hyperparametrs where used before and after tuning
- fix order with predictors selected, etc.



# questions for class
vszone?
what could be problem for vmnet?

